---
layout: post
title:  "Application Front End"
date:   2020-12-21 20:24:38 +0100
categories: blog post



---

Apologies for the length between posts, I had a surgery 4th of December, it slowed me down more than expected.

While this project could be seen primarily as a research investigation I would like to create a dashboard, or front end which displays the history of predictions of the best performing models tested and, if possible, current predictions based on live data.

Acquiring live data and setting up scripts to run the algorithms live may be too ambitious in the time frame, so will be left for a later blog post/ consideration

In this blog post I'll run through my options and thoughts on creating and displaying the historical data using various methods.

## General Options

### Jupyter Notebooks

The simplest option. A set of notebooks set up to store the best models and process past data, or more up to date provided data with frozen models. Matplotlib and Numpy can be used to display and interpret the predictions of the various models and their efficacy.

This will probably be the form of the first step of the process and may be sufficient if I wish to focus on more research oriented questions, however is not a working front end usable by many people and requires the user to collect the relevant data themselves.

### Python GUI

Python GUI libraries could be used both to display past predictions and to facilitate users to input more recent data and run them through the best performing models. Tools could be created to take some of the work out of processing the data, however it would still require heavy user involvement to collect both price and twitter data to feed into the models for updated predictions.

It may be possible to create scripts that gather the most recent data for processing to generate predictions, however the models themselves will most likely be locked into the most recently updated form and will stop training with the last update.

Examples are Tkinter and Flexx:

https://docs.python.org/3/library/tkinter.html

https://flexx.readthedocs.io/en/stable/

### Web Application

Would use either a database of past data or stored files to pass into the models. This option is probably the most involved, but opens up the possibility of creating scripts to collect data (in real-time) and generate predictions without user involvement. The models themselves could potentially be set up to continually train as new data comes in (though I am not entirely certain if this is possible or feasible).

The web app could be built in Node.js with modules installed to allow the use of python files and matplotlib, or it could be built in Flask which may make the process easier.

below is a relevant blog post to using live models:

https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86

## My choice

As mentioned, this project will in all likelihood start as a series of Jupyter notebooks, however I would like to create a web app interface to either explore the data generated by the models, or to display live predictions.

I have familiarity with web app development from the group project last year and it seems a good option to present findings to users. An application designed to run online should also open the possibility of running the models on live predictions. There is still some time before the final decision needs to be made and I will still explore the option of using a python GUI library instead.